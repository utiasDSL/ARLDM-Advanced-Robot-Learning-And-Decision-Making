{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Deep Reinforcement Learning (DRL)\n",
    "\n",
    "In DRL, we do not require a symbolic model of our system, only a (highly-parallizable) simulation. The policy is then learned directly from the data collected of the simulated robot interacting with its environment. As much data is needed for training (typically millions of samples), the efficient simulation implemented by [crazyflow](https://github.com/utiasDSL/crazyflow) will come in handy.\n",
    "\n",
    "In the lecture you, you were introduced to the **on-policy DRL PPO algorithm**, as it has become the most popular for robotics applications. In this exercise, you will use **PPO** to learn to fly a flying a figure eight trajectory. With a drone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 1: WandB</h3>\n",
    "    <p>\n",
    "    Go to <a href=\"https://wandb.ai/site\" target=\"_blank\">https://wandb.ai/site</a> and create a free WandB account. WandB is a very common tool in tracking deep learning experiments, and we will use it to track our training in this exercise. WandB includes 100GB of free online storage, whis is more than enough for our use case. Once you have created your account, execute the following code cell. Follow the instructions to obtain your API key and paste it as requested. If everything was successful, you should see a massage similar to <code>\"wandb: Currently logged in as: ...</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from ml_collections import ConfigDict\n",
    "from ppo import PPOTrainer, set_seeds\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 2 (Optional): Setup GPU</h3>\n",
    "    <p>\n",
    "    This exercise benefits from running the training on a powerful <a href=\"https://developer.nvidia.com/cuda-gpus\" target=\"_blank\">cuda-enabled GPU</a>. If you haven't done so yet, you can easily setup your container to run on the GPU. Consult the <code>README.md</code> for the instruction.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {train_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on  (CPU / GPU)**: There are two bottlenecks that determine the speed of any DRL training: a) The simulation (data collection) and b) the learning of the agent (optimization). Both can have there on device to run on, i.e., CPU or GPU. Unfortunately, choosing a device for a simulation is not always straightforward, as the simulation can be faster on either device, depending on the specific simulation and parallelization. For reference see [the example in MJX here](https://mujoco.readthedocs.io/en/stable/mjx.html#mjx-the-sharp-bits). If the simulation is run on CPU, it can make sense to run the optimization on the CPU as well, as moving tensors from CPU to GPU takes time.\n",
    "\n",
    "For crazyflow, we observe that simulation on GPU is faster if you run more than ~32 environments in parallel (depending on the specific CPU and GPU). A typical number of parallel environments to use for DRL is for instance 4096. At this point, the GPU provides significant speed ups as crazyflow is implemented in a way that the tensors stay on the GPU throughout the entire training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">As we will execute the entirety of the training code using a central config object defined below, all implementations are contained in external `.py` files. Make use of the local testing feature to test your implementations for the below tasks to make sure the training runs smoothly in the end!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 3: Deterministic Seeding for Reproducible PPO Training</h3>\n",
    "    <p>\n",
    "      In the provided <code>ppo.py</code>, implement the <code>set_seeds</code> function so that it ensures complete reproducibility across all sources of randomness.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Specifically, your function must:\n",
    "\n",
    "  - Set the Python built-in random module’s seed.\n",
    "\n",
    "  - Seed NumPy’s random number generator.\n",
    "\n",
    "  - Seed PyTorch’s CPU and all GPU (CUDA) random generators.\n",
    "\n",
    "  - Configure torch.backends.cudnn for fully deterministic behavior by enabling `deterministic = True` and disabling `benchmark = False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 4: Implement make_envs</h3>\n",
    "    <p>\n",
    "      Implement the <code>make_envs</code> function in <code>ppo.py</code> to construct two parallel Gymnasium vector environments.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 5: Exam Preparation: Normalization</h3>\n",
    "    <p>\n",
    "        Why do we need to normalize observations? (The reason is the same as for Neural Networks.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_model` function packages everything needed to resume training or run inference—namely the agent’s network weights, optimizer state, and the environment’s normalization statistics—into a single checkpoint file. This ensures you can stop and later restart training seamlessly, and that input normalization remains consistent at test time.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 6: Implement save_model</h3>\n",
    "    <p>\n",
    "      Implement the <code>save_model</code> function in <code>ppo.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 7: Check Code and Exam Preparation</h3>\n",
    "   <p>Have a look at the implementation of <code>Agent</code> class in <code>agent.py</code>. Answer the following questions:\n",
    "   \n",
    "   - Network initialization: Why do you have to be careful with the way you initialize the policy network? Have a look at <a href=\"https://arxiv.org/pdf/2006.05990\">https://arxiv.org/pdf/2006.05990</a> at page 5 to answer the question.\n",
    "   - In the <code>ppo.py</code> file, in <code>PPOTrainer.learn</code>, you can see an `entropy_loss` term in the computation of the overall loss (`loss = ...`). This entropy is computed in the `action_and_value` function in `agent.py` from the action probabilities. What would you expect to happen during training if you increase the weight of the entropy factor in the loss function (`self.config.ent_coef`)? For this, it is important that you know what the entropy of a distribution represents. You can look it up <a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\">on wikipedia</a>. <a href=\"https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\">This article</a> under point 10. also helps!\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 8: Generalized Advantage Estimate</h3>\n",
    "    <p>\n",
    "      You already learned about the PPO loss in the lecture. The PPO loss calculation requires the calculation of so-called Advantages. For an explanation, see the Script in Eq. 8.39, 8.40, and 8.42. Calculating the advantages is a critical design decision in DRL algorithms as it heavily influences the learning. A standard method is the Generalized Advantage Estimate (GAE).\n",
    "<p></p>\n",
    "      GAE is a bit more involved and we already implemented a part of it. However, you task is to complete the function. Head over to <code>ppo.py</code> and implement the <code>calculate_advantages</code> method of the <code>PPOTrainer</code> class. You will need to refer to one equation in the original GAE paper: <a href=\"https://arxiv.org/pdf/1506.02438\">https://arxiv.org/pdf/1506.02438</a>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 9: PPO-Clip Policy Gradient Loss</h3>\n",
    "    <p>\n",
    "      Go to the <code>ppo.py</code>, implement the <code>calculate_pg_loss</code> method of <code>PPOTrainer</code> class to compute the policy gradient loss.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Value Function Loss\n",
    "\n",
    "####  Case 1: **Unclipped Value Loss**\n",
    "\n",
    "When `if_clip = False`, the value loss is computed using the standard Mean Squared Error (MSE) between predicted values and target returns:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_v = \\frac{1}{2} \\cdot \\mathbb{E}_t \\left[ \\left( V_{\\theta}(s_t) - \\hat{R}_t \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "- $ V_{\\theta}(s_t) $: Current value function prediction (`newvalue`)\n",
    "- $ \\hat{R}_t $: Target return at time step $ t $ (from GAE, i.e., `b_returns`)\n",
    "- The factor $ \\frac{1}{2} $ is conventional in MSE to simplify derivative expressions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 2: **Clipped Value Loss**\n",
    "\n",
    "When `if_clip = True`, PPO applies clipping to the value function update to prevent large deviations from the old value estimate.\n",
    "\n",
    "\n",
    "##### **Step 1: Clipped Value Prediction**\n",
    "\n",
    "$$\n",
    "v_t^{\\text{clip}} = V_{\\theta_{\\text{old}}}(s_t) + \\text{clip}\\left(V_{\\theta}(s_t) - V_{\\theta_{\\text{old}}}(s_t),\\; -\\epsilon,\\; +\\epsilon \\right)\n",
    "$$\n",
    "\n",
    "##### **Step 2: Loss Calculation**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell_{\\text{unclip}} &= \\left(V_{\\theta}(s_t) - \\hat{R}_t\\right)^2 \\\\\n",
    "\\ell_{\\text{clip}} &= \\left(v_t^{\\text{clip}} - \\hat{R}_t\\right)^2 \\\\\n",
    "\\mathcal{L}_v &= \\frac{1}{2} \\cdot \\mathbb{E}_t \\left[ \\max\\left( \\ell_{\\text{unclip}},\\; \\ell_{\\text{clip}} \\right) \\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 10: Value Loss</h3>\n",
    "   <p> Go to the <code>ppo.py</code>, complete the implementation of <code>calculate_v_loss</code> method of <code>PPOTrainer</code> class.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 11: Check code</h3>\n",
    "   <p> The <code>learn</code> method is the core part of the <code>PPO</code> algorithm — it is responsible for training both the policy and value networks. The method performs multiple optimization steps using previously collected experience samples (such as <code>observations, actions, log probabilities, returns, etc.</code>).</p>\n",
    "\n",
    "   <p>Have a look at the <code>loss=...`</code> line in the <code>`learn()`</code> function, where all loss components are accumulated.</p>\n",
    "\n",
    "After completing the previous tasks, you are encouraged to take a closer look at the overall workflow inside the <code>learn</code> method.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h3>Task 12: Train Your Agent</h3>\n",
    "  <p>\n",
    "    Once you’ve completed the previous tasks, run the following cell to train your PPO agent. \n",
    "    We will first evaluate your algorithm implementation using the provided \n",
    "    <code>DroneReachPos</code> environment. \n",
    "    You can check its source code \n",
    "    (<a href=\"https://github.com/utiasDSL/crazyflow/blob/main/crazyflow/envs/reach_pos_env.py\" target=\"_blank\">reach_pos_env.py</a>) \n",
    "    to understand the task objectives and reward structure. \n",
    "    If your PPO implementation is correct, you should be able to achieve the target reward \n",
    "    and pass the public test.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    Please make sure your algorithm works correctly at this stage! \n",
    "    In the following tasks, you’ll use your PPO implementation in more challenging environments, where you’ll experiment with custom environment setups and reward shaping.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfigDict allows us to use convinient dot-based property access: https://github.com/google/ml_collections\n",
    "train_config = ConfigDict(\n",
    "    {\n",
    "        \"env_name\": \"DroneReachPos-v0\",\n",
    "        \"max_episode_time\": 10.0,\n",
    "        \"n_envs\": 1024,\n",
    "        \"device\": train_device,\n",
    "        \"total_timesteps\": 1_200_000,\n",
    "        \"learning_rate\": 1.5e-3,\n",
    "        \"n_steps\": 16,  # Number of steps per environment per policy rollout\n",
    "        \"gamma\": 0.90,  # Discount factor\n",
    "        \"gae_lambda\": 0.95,  # Lambda for general advantage estimation\n",
    "        \"n_minibatches\": 16,  # Number of mini-batches\n",
    "        \"n_epochs\": 15,\n",
    "        \"norm_adv\": True,\n",
    "        \"clip_coef\": 0.25,\n",
    "        \"clip_vloss\": True,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 5.0,\n",
    "        \"target_kl\": None,\n",
    "        \"seed\": 0,\n",
    "        \"n_eval_envs\": 32,\n",
    "        \"n_eval_steps\": 500,\n",
    "        \"save_model\": True,\n",
    "        \"eval_interval\": 999_000,\n",
    "        \"lr_decay\": True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "# NOTE: if you see \"Broken pipe\" error, it's probably a wandb issue, restart kernal and start over.\n",
    "set_seeds(train_config.seed)\n",
    "trainer = PPOTrainer(train_config, wandb_log=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 13: Test  your Agent</h3>\n",
    "    <p>\n",
    "        Once training has finished, run the inference using the cell below. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import PPOTester\n",
    "\n",
    "path = Path.cwd() / \"ppo_checkpoint_DroneReachPos-v0.pt\"\n",
    "\n",
    "# set render to `True` to see how the agent performs.\n",
    "_ = PPOTester(seed=42, ckpt_path=path, n_episodes=5, render=True, env_name=\"DroneReachPos-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 14: Transfer to the Figure 8 Trajectory Tracking Task</h3>\n",
    "  <p>\n",
    "    Previously, we trained a policy capable of reaching arbitrary goal positions. \n",
    "    In theory, such a policy should also be able to follow continuous trajectories. \n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    In this task, you will work with the <code>DroneFigureEightTrajectory-v0</code> environment.\n",
    "    Check the original implementation here: \n",
    "    (<a href=\"https://github.com/utiasDSL/crazyflow/blob/main/crazyflow/envs/figure_8_env.py\" target=\"_blank\">figure_8_env.py</a>) \n",
    "    Run the cell below to evaluate how your policy performs on the figure 8 trajectory.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    Check the <code>eval_plot.png</code> file in the exercise folder to view the quantitative results. \n",
    "    The plot includes actions, positions, orientations, and the tracking RMSE.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import PPOTester\n",
    "\n",
    "# reuse our policy for position reaching\n",
    "path = Path.cwd() / \"ppo_checkpoint_DroneReachPos-v0.pt\"\n",
    "\n",
    "# see how the reach position agent performs on trajectory tracking task.\n",
    "_ = PPOTester(seed=0, ckpt_path=path, n_episodes=1, render=True, env_name=\"DroneFigureEightTrajectory-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 15: Train the Policy on the Figure 8 Trajectory Tracking Task</h3>\n",
    "  <p>\n",
    "    You may notice that while the current policy can follow the trajectory by tracking goal positions, \n",
    "    it consistently lags behind. This happens because the policy is unaware of the upcoming \n",
    "    trajectory shape and receives no velocity commands—preventing it from anticipating future motion.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    To address this, you can sample multiple upcoming points from the trajectory and include them \n",
    "    in the observation space, allowing the policy to understand the trajectory’s future direction. \n",
    "    Once again, refer to <code>eval_plot.png</code> for quantitative tracking results.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import PPOTrainer\n",
    "\n",
    "# Train policy in figure 8 tracking task\n",
    "train_config = ConfigDict(\n",
    "    {\n",
    "        \"env_name\": \"DroneFigureEightTrajectory-v0\",\n",
    "        \"max_episode_time\": 10.0,\n",
    "        \"n_envs\": 1024,\n",
    "        \"device\": train_device,\n",
    "        \"total_timesteps\": 1_000_000,\n",
    "        \"learning_rate\": 1.5e-3,\n",
    "        \"n_steps\": 16,  # Number of steps per environment per policy rollout\n",
    "        \"gamma\": 0.90,  # Discount factor\n",
    "        \"gae_lambda\": 0.95,  # Lambda for general advantage estimation\n",
    "        \"n_minibatches\": 16,  # Number of mini-batches\n",
    "        \"n_epochs\": 15,\n",
    "        \"norm_adv\": True,\n",
    "        \"clip_coef\": 0.25,\n",
    "        \"clip_vloss\": True,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 5.0,\n",
    "        \"target_kl\": None,\n",
    "        \"seed\": 0,\n",
    "        \"n_eval_envs\": 128,\n",
    "        \"n_eval_steps\": 500,\n",
    "        \"save_model\": True,\n",
    "        \"eval_interval\": 999_000,\n",
    "        \"lr_decay\": True,\n",
    "    }\n",
    ")\n",
    "set_seeds(train_config.seed)\n",
    "# Now we set n_samples=10 to include 10 points ahead of the current goal position\n",
    "trainer = PPOTrainer(train_config, wandb_log=True, n_samples=10)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import PPOTester\n",
    "\n",
    "# load trajectory tracking task environment\n",
    "train_config.env_name = \"DroneFigureEightTrajectory-v0\"\n",
    "# reuse our policy for position reaching\n",
    "path = Path.cwd() / \"ppo_checkpoint_DroneFigureEightTrajectory-v0.pt\"\n",
    "\n",
    "# set render to `True` to see how the agent performs.\n",
    "_ = PPOTester(seed=10, ckpt_path=path, n_episodes=1, render=True, env_name=\"DroneFigureEightTrajectory-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 16: Transfer Policy to Random Trajectory Tracking</h3>\n",
    "  <p>\n",
    "    Up to this point, your policy should be able to track the figure-eight trajectory effectively. \n",
    "    However, there is still only one fixed trajectory. In this task, you will extend your policy to track \n",
    "    <strong>randomly generated trajectories</strong>.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    We have a hidden test dataset containing such random trajectories, and your policy is expected \n",
    "    to generalize to these unseen cases. The dataset itself will <strong>not</strong> be provided to you. \n",
    "    In the cells below, we include one example of a random trajectory that you can use to evaluate \n",
    "    your current policy’s performance locally for faster iteration.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    You should review and modify the code in <code>rand_traj_env.py</code>.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    This file includes an implementation of the figure-eight trajectory tracking environment. \n",
    "    Feel free to customize any part of it to train your policy, including the \n",
    "    <code>observations</code>, <code>reset</code>, <code>reward</code>.\n",
    "    You can also modify the neural network architecture in the <code>MyAgent</code> class if needed.\n",
    "    In addition, you are encouraged to experiment with different \n",
    "    <strong>hyperparameters</strong> in train_config dict.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    Your objective is to train a policy that achieves an \n",
    "    <strong>RMSE below 100&nbsp;mm</strong> on our hidden test set.\n",
    "    Make sure to <strong>commit your trained policy</strong> <code>ppo_checkpoint_DroneRandomTrajectory-v0.pt</code> to Artemis,\n",
    "    as it will be evaluated during the final testing phase.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"img/rand_traj_examples.png\" alt=\"Random Trajectory Examples\" width=\"1600\">\n",
    "\n",
    "<p><em>This figure shows examples of the random trajectories you will encounter during the online test.</em></p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See how figure 8 policy generalize on random trajectory\n",
    "from ppo import PPOTester  # noqa: I001\n",
    "from rand_traj_env import RandTrajEnv\n",
    "from gymnasium.envs.registration import register\n",
    "from exercise06.wrappers import RandTrajTestWrapper\n",
    "from exercise06.ppo import make_envs\n",
    "import pickle\n",
    "\n",
    "# load sample trajectroy\n",
    "filepath = Path.cwd().parent / \"test/behavior/exercise06_testdata.pkl\"\n",
    "with open(filepath, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# reuse our policy for figure 8 trajectory\n",
    "path = Path.cwd() / \"ppo_checkpoint_DroneFigureEightTrajectory-v0.pt\"\n",
    "\n",
    "# Load student environment and wrap it with testing wrapper\n",
    "register(id=\"DroneRandomTrajectory-v0\", vector_entry_point=RandTrajEnv)\n",
    "_, test_env = make_envs(\"DroneRandomTrajectory-v0\", 1, 1, \"cpu\")\n",
    "test_env = RandTrajTestWrapper(test_env, trajectory=data[\"random_trajectories\"][\"example_trajectory\"])\n",
    "\n",
    "# set render to `True` to see how the agent performs.\n",
    "reward, rmse = PPOTester(seed=42, ckpt_path=path, n_episodes=1, render=True, test_env=test_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is for training on your customized environment\n",
    "from ppo import PPOTrainer  # noqa: I001\n",
    "from rand_traj_env import RandTrajEnv, MyAgent\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(id=\"DroneRandomTrajectory-v0\", vector_entry_point=RandTrajEnv)\n",
    "# Train policy in spline tracking task\n",
    "train_config = ConfigDict(\n",
    "    {\n",
    "        \"env_name\": \"DroneRandomTrajectory-v0\",\n",
    "        \"max_episode_time\": 10.0,\n",
    "        \"n_envs\": 1024,\n",
    "        \"device\": train_device,\n",
    "        \"total_timesteps\": 1_000_000,\n",
    "        \"learning_rate\": 1.5e-3,\n",
    "        \"n_steps\": 16,  # Number of steps per environment per policy rollout\n",
    "        \"gamma\": 0.90,  # Discount factor\n",
    "        \"gae_lambda\": 0.95,  # Lambda for general advantage estimation\n",
    "        \"n_minibatches\": 16,  # Number of mini-batches\n",
    "        \"n_epochs\": 15,\n",
    "        \"norm_adv\": True,\n",
    "        \"clip_coef\": 0.25,\n",
    "        \"clip_vloss\": True,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 5.0,\n",
    "        \"target_kl\": None,\n",
    "        \"seed\": 0,\n",
    "        \"n_eval_envs\": 128,\n",
    "        \"n_eval_steps\": 500,\n",
    "        \"save_model\": True,\n",
    "        \"eval_interval\": 999_000,\n",
    "        \"lr_decay\": True,\n",
    "    }\n",
    ")\n",
    "set_seeds(train_config.seed)\n",
    "trainer = PPOTrainer(train_config, wandb_log=True, agent_cls=MyAgent, n_samples=10)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for you to test your policy on your customized environment\n",
    "from gymnasium.envs.registration import register\n",
    "from ppo import PPOTester\n",
    "from rand_traj_env import MyAgent, RandTrajEnv\n",
    "\n",
    "register(id=\"DroneRandomTrajectory-v0\", vector_entry_point=RandTrajEnv)\n",
    "# load trajectory tracking task environment\n",
    "train_config.env_name = \"DroneRandomTrajectory-v0\"\n",
    "path = Path.cwd() / \"ppo_checkpoint_DroneRandomTrajectory-v0.pt\"\n",
    "\n",
    "# set render to `True` to see how the agent performs.\n",
    "_ = PPOTester(seed=42, ckpt_path=path, n_episodes=2, render=True, agent_cls=MyAgent, env_name=\"DroneRandomTrajectory-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 17: Test Your Policy on the Random Trajectory Tracking Task</h3>\n",
    "  <p>\n",
    "    We provide a test wrapper that loads a sample trajectory from our hidden test dataset. \n",
    "    You should ensure that your customized environment runs correctly with the test code below.\n",
    "  </p>\n",
    "  <p><strong>Note:</strong></p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      Achieving an RMSE below <strong>100&nbsp;mm</strong> on this sample test \n",
    "      does <em>not</em> guarantee that your policy will pass the private test online!\n",
    "    </li>\n",
    "    <li>\n",
    "      Again, remember to <strong>commit your trained policy</strong> \n",
    "      (<code>ppo_checkpoint_DroneRandomTrajectory-v0.pt</code>) to Artemis.\n",
    "    </li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See how trained policy performs on random trajectory\n",
    "from ppo import PPOTester  # noqa: I001\n",
    "from rand_traj_env import RandTrajEnv, MyAgent\n",
    "from gymnasium.envs.registration import register\n",
    "from exercise06.wrappers import RandTrajTestWrapper\n",
    "from exercise06.ppo import make_envs\n",
    "import pickle\n",
    "\n",
    "# load sample trajectroy\n",
    "filepath = Path.cwd().parent / \"test/behavior/exercise06_testdata.pkl\"\n",
    "with open(filepath, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "path = Path.cwd() / \"ppo_checkpoint_DroneRandomTrajectory-v0.pt\"\n",
    "\n",
    "# Load student environment and wrap it with test wrapper\n",
    "register(id=\"DroneRandomTrajectory-v0\", vector_entry_point=RandTrajEnv)\n",
    "_, test_env = make_envs(\"DroneRandomTrajectory-v0\", 1, 1, \"cpu\")\n",
    "test_env = RandTrajTestWrapper(test_env, trajectory=data[\"random_trajectories\"][\"example_trajectory\"])\n",
    "\n",
    "# set render to `True` to see how the agent performs.\n",
    "reward, rmse = PPOTester(seed=42, ckpt_path=path, n_episodes=1, render=True, agent_cls=MyAgent, test_env=test_env)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
