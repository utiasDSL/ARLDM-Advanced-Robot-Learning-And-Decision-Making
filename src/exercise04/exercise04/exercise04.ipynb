{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Model Learning\n",
    "\n",
    "> Welcome to the Advanced Robot Learning and Decision Making exercises!\n",
    "\n",
    "In this exercise, we will review **model learning**, a fundamental concept in robot learning. Specifically, we will focus on learning the dynamics of a quadrotor from data using two popular model types: **Gaussian Processes (GPs)** and **Neural Networks (NNs)**.\n",
    "\n",
    "Model learning involves approximating the underlying dynamics of a system based on observed data. This is crucial for tasks such as control, planning, and simulation in robotics. By learning a model of the system's dynamics, we can predict how the system will behave and use that information to improve decision-making and control.\n",
    "\n",
    "### Exercise Overview\n",
    "\n",
    "1. **Data Collection and Preprocessing**\n",
    "   \n",
    "   Collect and preprocess state transition data from a simulated quadrotor. We provide a raw dataset for you to use.\n",
    "\n",
    "2. **Gaussian Processes**\n",
    "  \n",
    "   Complete the implementation of a multi-output GP model and train it. Evaluate the GP model's performance and analyze its uncertainty estimates.\n",
    "\n",
    "3. **(Optional) Introduction to Deep Learning**\n",
    "\n",
    "   Pytorch, autograd, and basic optimization.\n",
    "\n",
    "3. **Neural Networks**\n",
    "   \n",
    "   Design and train a NN for the same task as you have done for GPs. Evaluate the NN model's performance and analyze its learning curves.\n",
    "\n",
    "4. **Performance Comparison**\n",
    "\n",
    "   Compare the performance of the GP and NN models in terms of accuracy, training time, and inference time. Discuss the trade-offs and practical \n",
    "   considerations for using each model type.\n",
    "\n",
    "\n",
    "_____\n",
    "\n",
    "By the end of this exercise, you will have a solid understanding of how to use Gaussian Processes and Neural Networks for learning dynamics from data, as well as the strengths and weaknesses of each approach.\n",
    "\n",
    "### Objectives of This Exercise\n",
    "1. **Gaussian Processes (GPs):**\n",
    "   - Understand the basics of GPs and their application in modeling dynamics.\n",
    "   - Learn how to implement and use GPs for multi-output regression.\n",
    "   - Explore the benefits of GPs, such as uncertainty quantification, and their limitations, such as scalability.\n",
    "\n",
    "2. **Neural Networks (NNs):**\n",
    "   - Understand the basics of NNs and their application in modeling dynamics.\n",
    "   - Learn how to design, train, and evaluate NNs for regression tasks.\n",
    "   - Explore best practices for training NNs, including normalization, regularization, and debugging learning curves.\n",
    "\n",
    "3. **Comparison of GPs and NNs:**\n",
    "   - Compare the performance of GPs and NNs in terms of accuracy, training time, and inference time.\n",
    "   - Discuss the trade-offs between these two model types and their suitability for different tasks.\n",
    "\n",
    "### Why Gaussian Processes and Neural Networks?\n",
    "- **Gaussian Processes** are non-parametric models that provide uncertainty estimates, making them ideal for tasks where understanding model confidence is critical. However, they can be computationally expensive for large datasets.\n",
    "- **Neural Networks** are parametric models that excel at handling large datasets and complex relationships. They are highly scalable but require careful tuning to avoid overfitting and ensure generalization.\n",
    "\n",
    "\n",
    "_____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from exercise04.data_collection import load_state_transitions\n",
    "from exercise04.gaussian_process import SVGPTrainer\n",
    "from exercise04.neural_network import NeuralNetwork\n",
    "from exercise04.plotting_utils import (\n",
    "    plot_2d_positions_with_std,\n",
    "    plot_error_distribution,\n",
    "    plot_grid,\n",
    "    plot_learning_curves,\n",
    "    plot_prediction_vs_truth,\n",
    "    plot_random_test_point,\n",
    "    plot_recorded_states,\n",
    "    plot_regression_results,\n",
    ")\n",
    "from exercise04.utils import (\n",
    "    CustomDataset,\n",
    "    Normalizer,\n",
    "    error_statistics,\n",
    "    generate_synthetic_data,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection and Preprocessing\n",
    "Data collection and processing are critical steps in any machine learning pipeline, especially in robotics, where the data quality is extremly critical and data collection often difficult. \n",
    "\n",
    "In this exercise, we collect state transitions from a simulated quadrotor drone flying in a noisy environment using a pretrained DRL policy (which you will see again in exercise 06). These transitions include sequences of states, actions, and the resulting next states. Of course, in practice data collection is done in real-world environments, not in simulation.\n",
    "\n",
    "Let's review some fundamental concepts in data collection and processing.\n",
    "### Data Collection\n",
    "- **\"Garbage in, Garbage out\"**: The performance of data-based methods is strongly dependent on the available data. If the data training is poor or unrepresentative, the resulting model will also perform poorly. However, it may not show that during training, which is why critically evaluating the learning process is an important task when training data-based models.\n",
    "<!-- - **Simulation vs Real-World Deployment**: Simulations are faster, easier, and safer compared to real-world data collection. They allow for controlled environments and reproducibility. However, simulations often lack the details and imperfections of real-world scenarios, such as sensor noise, environmental variability, and unexpected dynamics. Simulators themselves are often built using real-world data to approximate reality. -->\n",
    "- **Importance of Data Diversity**: Diversity in the collected data is key to preventing overfitting and enabling generalization. A diverse dataset ensures that the model can handle a wide range of scenarios, including varying initial conditions, different trajectories or behaviors, and environmental factors.\n",
    "\n",
    "### Data Preprocessing\n",
    "Before training a model, data is almost always processed to improve learning efficiency and model performance. Key preprocessing steps include:\n",
    "\n",
    "1. **Normalization**: \n",
    "   - Normalizing input features (e.g., states and actions) ensures that all features have similar scales. This prevents certain features from dominating the learning process and often significantly improves the performance of NNs and other data-based models.\n",
    "\n",
    "2. **Data Augmentation**:\n",
    "   - Augmenting the data by adding noise, rotations, scaling, or other transformations can increase the effective dataset size and improve the model's robustness to variations. This is particularly useful when the available dataset is small.\n",
    "\n",
    "3. **Data Splitting**:\n",
    "   - Splitting the dataset into training, validation, and test sets is crucial for evaluating model performance. The training set is used to train the model, the validation set is used to tune hyperparameters and prevent overfitting, and the test set is used to assess the final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state transitions (before check whether exercise04/outputs/state_transitions.pkl exists and pull from git if not)\n",
    "recorded_data = load_state_transitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the drone's x and y positions to verify how the drone flew. As you can see, we added noise to both actions and state to increase the fitting difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_names = [\"pos_x\",\"pos_y\",\"pos_z\",\"vel_x\",\"vel_y\",\"vel_z\",\"quat_w\",\"quat_x\",\"quat_y\",\"quat_z\",\"ang_vel_x\",\"ang_vel_y\", \"ang_vel_z\"]\n",
    "\n",
    "plot_recorded_states(recorded_data[\"states\"], state_names=[\"pos_x\",\"pos_z\"])  # Visualize recorded states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract the features (state and actions) and the targets (next states) from the recorded data. Then we split the data into training- and test-sets using the sklearn library. As we are not doing any automatic hyperparameter tuning, we do not need a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y datasets from recorded_data\n",
    "x_drone = np.concatenate((recorded_data[\"states\"], recorded_data[\"actions\"]), axis=1)\n",
    "y_drone = recorded_data[\"next_states\"]  # we will need those variables later\n",
    "\n",
    "# Perform train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_drone, y_drone, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "raw_data = {\"x_train\": x_train, \"x_test\": x_test, \"y_train\": y_train, \"y_test\": y_test}\n",
    "\n",
    "# Print the shapes of the train and test datasets\n",
    "for key, value in raw_data.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gaussian Process Regression\n",
    "\n",
    "Gaussian Processes (GPs) are powerful tools for regression tasks, especially in robotics, where quantifying uncertainty is crucial. In this section, we review the fundamentals of Gaussian Processes and regression, introduce the task of implementing a custom multi-output GP model, and discuss the difference between learning complete dynamics and residual dynamics.\n",
    "\n",
    "### Basics of Gaussian Processes\n",
    "- **Gaussian Processes** are non-parametric models that define a distribution over functions. They are particularly useful for regression because they provide both predictions and associated uncertainty estimates.\n",
    "- A GP is fully specified by a **mean function** and a **kernel (covariance) function**:\n",
    "  - The **mean function** gives the expected value of the function at any input.\n",
    "  - The **kernel function** measures similarity between inputs and determines the smoothness and complexity of the function.\n",
    "- GPs are best suited for small to medium-sized datasets due to their computational complexity, which scales cubically with the number of data points.\n",
    "\n",
    "### Task Overview: Implementing a Multi-Output Gaussian Process Model\n",
    "In this section, you will:\n",
    "1. Implement a custom multi-output GP model.\n",
    "2. Fit the GP model to previously collected training data.\n",
    "3. Evaluate the model’s performance by predicting the mean and uncertainty of the next states.\n",
    "\n",
    "The implementation involves:\n",
    "- Defining a **squared exponential (SE) kernel**, commonly used for its smoothness properties.\n",
    "- Completing the **predict()** function to compute the posterior mean and variance for new inputs.\n",
    "\n",
    "### Learning Complete Dynamics vs. Residual Dynamics\n",
    "When modeling system dynamics, two common approaches are:\n",
    "\n",
    "1. **Learning Complete Dynamics**:\n",
    "   - The GP learns the entire mapping from the current state and action to the next state.\n",
    "   - This approach typically requires more data and is computationally more expensive.\n",
    "\n",
    "2. **Learning Residual Dynamics**:\n",
    "   - If a base model $f_\\mathrm{base}$ is available (e.g., a physics-based model or an informed guess), the GP learns the residuals $y_\\mathrm{res} = y - f_\\mathrm{base}(x)$.\n",
    "   - This approach is often more stable and data-efficient, as the GP only models the discrepancies between the base model and observed data.\n",
    "\n",
    "In this exercise, we will focus on learning the complete dynamics due to a simpler implementation. However, in practice, learning residual dynamics is often preferred when a reliable base model is available. The next exercise (GP-MPC) will deal with such a residual model learning setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exam Preparation</h3>\n",
    "    <p>\n",
    "    Review the lecture note sections that consider Gaussian Processes (GPs). Then discuss important GP parameters and look up which kernels exist and when they are typically used.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 1: Review and complete the BaseGaussianProcess class</h3>\n",
    "    Review the class <code>BaseGaussianProcess</code> in <code>gaussian_process.py</code>.\n",
    "    <p>\n",
    "    Implement the <code>se_kernel()</code> and <code>linear_kernel()</code> functions, and at least one additional kernel of your choice (e.g., linear, polynomial, etc.).  The kernel parameters must be provided during class initialization and newly implemented kernels must be registered in the <code>__init__()</code> function.\n",
    "    <p>\n",
    "    Afterwards complete first the <code>fit()</code> function, which computes the kernel matrix, the dual coefficients, and the Cholesky decomposition from the training data, and then the <code>predict()</code> function that predicts the posterior mean and (optionally) variance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise04.gaussian_process import MultiOutputGaussianProcess  # noqa: I001\n",
    "# TODO: Define the kernel parameters\n",
    "kernel_params = {\"length_scale\": 0.25, \"nu\": 1.5}\n",
    "gp_args = {\n",
    "    \"kernel\": \"matern\",\n",
    "    \"kernel_params\": kernel_params,\n",
    "    \"noise\": 1e-5,\n",
    "    \"max_samples\": 2000, # Limit the number of training samples to 2000, if code crashes try limiting further\n",
    "}\n",
    "# Fit a Gaussian Process to the data\n",
    "gp_results = {}\n",
    "gp = MultiOutputGaussianProcess(**gp_args)\n",
    "start = time.time()\n",
    "gp.fit(raw_data[\"x_train\"], raw_data[\"y_train\"])\n",
    "gp_results[\"t_train\"] = time.time() - start\n",
    "print(f\"GP training took {gp_results['t_train']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set\n",
    "start = time.time()\n",
    "gp_results[\"y_pred\"], gp_results[\"y_std\"] = gp.predict(raw_data[\"x_test\"], return_std=True)\n",
    "gp_results[\"t_pred\"] = time.time() - start\n",
    "print(f\"GP inference took {gp_results['t_pred']}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute error and squared error\n",
    "gp_results[\"mae\"], gp_results[\"rmse\"], gp_results[\"uncertainty\"] = error_statistics(\n",
    "    raw_data[\"y_test\"], gp_results[\"y_pred\"], gp_results[\"y_std\"]\n",
    ")\n",
    "print(f\"GP MAE: {gp_results['mae']}, RMSE: {gp_results['rmse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inspect results via plotting. The plotting functions are in <code>plotting_utils.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define lists convinient for plotting\n",
    "state_names = [\n",
    "    \"pos_x\",\n",
    "    \"pos_y\",\n",
    "    \"pos_z\",\n",
    "    \"vel_x\",\n",
    "    \"vel_y\",\n",
    "    \"vel_z\",\n",
    "    \"quat_w\",\n",
    "    \"quat_x\",\n",
    "    \"quat_y\",\n",
    "    \"quat_z\",\n",
    "    \"ang_vel_x\",\n",
    "    \"ang_vel_y\",\n",
    "    \"ang_vel_z\",\n",
    "]\n",
    "action_names = [\"thrust\", \"roll\", \"pitch\", \"yaw\"]\n",
    "\n",
    "# Visualize GP regression results in a grid\n",
    "plot_grid([\n",
    "    lambda ax: plot_random_test_point(\n",
    "        raw_data[\"y_test\"], gp_results[\"y_pred\"], gp_results[\"y_std\"],\n",
    "        state_1_name=\"pos_x\", state_2_name=\"pos_z\", axis=ax\n",
    "    ),\n",
    "    lambda ax: plot_2d_positions_with_std(\n",
    "        raw_data[\"y_train\"], raw_data[\"y_test\"], gp_results[\"y_pred\"], gp_results[\"y_std\"],\n",
    "        state_1_name=\"pos_x\", state_2_name=\"pos_z\", show_train=False,\n",
    "        x_lim=(-0.4, -0.1), y_lim=(0.5, 0.6), axis=ax\n",
    "    ),\n",
    "    lambda ax: plot_prediction_vs_truth(\n",
    "        raw_data[\"y_test\"], gp_results[\"y_pred\"], gp_results[\"y_std\"],\n",
    "        feature_name=\"pos_x\", model_name=\"GP\", axis=ax\n",
    "    ),\n",
    "    lambda ax: plot_error_distribution(\n",
    "        raw_data[\"y_test\"], gp_results[\"y_pred\"], state_names, model_name=\"GP\", axis=ax\n",
    "    ),\n",
    "], titles=[\n",
    "    \"Random Test Point\",\n",
    "    \"2D Position Prediction\",\n",
    "    \"Prediction vs Truth (pos_x)\",\n",
    "    \"Error Distribution\"\n",
    "], grid=(2,2), figsize=(12,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exam Preparation</h3>\n",
    "    <p>\n",
    "    Review the GP Prediction vs Ground Truth plot and answer following questions.\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li>Why are there certain regions where the model confidence decreases significantly? (There is a precise answer for our dataset.)</li>\n",
    "        <li>What could be done to counteract that?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 (Optional) Introduction to Deep Learning: Pytorch, Autograd and Basic Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more involved optimization problems that require gradient computation, we usually use Pytorch. Pytorch can be described as \"numpy for the GPU\" and enables efficient parallelization with GPU usage as well as automatic gradient computation (autograd). Array in pytorch are called tensors. \n",
    "For an introduction, check out parts 0, 1, 2, 5 and 6 of the [pytorch guide](https://pytorch.org/tutorials/beginner/basics/intro.html#how-to-use-this-guide). We really recommend checking out those tutorials as you will be working a lot with pytorch if you pursue deep learning applications. It will also help in our deep reinforcement learning exercises later on. In the following we will review some basic features of pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create basic operations, just as in numpy. Arrays (numpy) are called tensors in PyTorch.\n",
    "\n",
    "# tensor creation\n",
    "a = torch.tensor([1, 2, 3, 4])  # from list\n",
    "b = torch.ones(2, 4)  # 2x4 matrix of ones\n",
    "c = a + b  # broadcasting, just like in numpy\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful feature of PyTorch are operations on the GPU.\n",
    "For this, tensors need to be moved to the GPU-device.\n",
    "By default, this devcontainer is set to work with CPU, which should be sufficient for this exercise\n",
    "However, you can easily switch to use the GPU. For this see the README.md of this repo. You will need a NVIDIA GPU in that case. Modern deep learning libraries almost exclusively run on the GPU due to vast performance speed ups.\n",
    "\n",
    "Note that your variables must be on the same device, if you do calculations with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect location of a tensor; default is cpu\n",
    "print(a.device)\n",
    "\n",
    "# move to GPU, if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    a = a.to(\"cuda\")\n",
    "    print(a.device)\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU instead.\")\n",
    "\n",
    "# move back to CPU\n",
    "a = a.to(\"cpu\")\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some basic operations in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping\n",
    "a = a.reshape(2, 2)\n",
    "b = b.reshape(-1, 2)  # -1 infers the size\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenation\n",
    "c = torch.cat([a, b], dim=0)  # stack a and b vertically\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing\n",
    "print(c[1, 1])  # get the element at row 1, column 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another powerful feature of PyTorch is [Autograd (automatic differentiation)](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). If you haven't done any Deep Learning course, we strongly recommend to checkout the linked tutorial. Autograd allows to automatically track gradients through the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(\n",
    "    [1.0, 2.0, 3.0], requires_grad=True\n",
    ")  # required_grad tells pytorch to track gradients (this is often enabled by default)\n",
    "\n",
    "# calculate a function on the input tensor\n",
    "y = torch.sin(x) * torch.sin(x)\n",
    "z = y.sum()  # thats our 'target' function\n",
    "\n",
    "z.backward()  # compute gradients in computational graph\n",
    "print(\"\\nGradient after sin²(x) and sum:\")\n",
    "print(\"Gradient with PyTorch: dx =\", x.grad)\n",
    "\n",
    "x.grad.zero_()  # reset gradients. This is important if you want to perform another backward pass later on.\n",
    "\n",
    "with torch.no_grad():  # disable gradient tracking (used for faster and less memory intense inference)\n",
    "    print(\"Manual analytical gradient: dx =\", 2 * torch.sin(x) * torch.cos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd can be used for gradient-based optimization (such as it is done for neural networks). In this example, we fit a parameterized basis function to some simple synthetic data. Note that is only an example on how a simple optimization can be computed. Training neural networks requires further strategies that we introduce later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random dataset\n",
    "X_np, y_np = generate_synthetic_data(n_samples=100, noise_level=0.1)\n",
    "# convert to PyTorch tensors\n",
    "X = torch.FloatTensor(X_np)\n",
    "y = torch.FloatTensor(y_np)\n",
    "\n",
    "# Construct a basic function. In our case its easy, since we know the functions used in generate_synthetic_data\n",
    "X_poly = torch.cat(\n",
    "    [\n",
    "        X,\n",
    "        X**2,\n",
    "        X**3,\n",
    "        X**4,\n",
    "        X**5,  # Polynomial features\n",
    "        torch.sin(6 * X),  # Sinusoidal basis\n",
    "        torch.exp((X - 0.5) ** 2),  # Sharper Gaussian\n",
    "    ],\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "# Define trainable weights\n",
    "# Our trainable function will be X_poly @ weights\n",
    "weights = torch.randn(X_poly.shape[1], 1, requires_grad=True)\n",
    "\n",
    "# plot the data and the initial prediction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_np, y_np, label=\"Training data\")\n",
    "plt.plot(X_np, X_poly @ weights.detach().numpy(), \"r-\", label=\"Prediction\")\n",
    "plt.title(\"Initial prediction\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a training loop, we iterate over the data (X) to optimize the weights of our target function to best fit the data.\n",
    "\n",
    "For the optimization, we require an optimizer and a loss function (the target). A good choice of optimizer is ADAM, and a typical loss function for regression is the L2-norm loss.\n",
    "\n",
    "Please check out how to use basic optimization loops if you havent worked with Deep Learning already [in the PyTorch tutorials](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html). You will need this knowledge when working with Deep Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "# An optimizer defines a routine of how the weights should be updated depending on the gradients. A key parameter is the learning rate: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "# The trainable weights need to be registered with the optimizer\n",
    "optimizer = torch.optim.Adam([weights], lr=0.05)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 500\n",
    "losses = []\n",
    "\n",
    "# one epoch is one full pass through the data\n",
    "# please read up on training loops if you haven't done so\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = X_poly @ weights\n",
    "    loss = torch.mean((y_pred - y) ** 2)  # L2 loss\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # reset gradients\n",
    "    loss.backward()  # calculate gradients\n",
    "    optimizer.step()  # perform parameter updates based on the gradients using the defined optimizer\n",
    "\n",
    "    losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    # Print progress every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Final fit\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_np, y_np, alpha=0.5, label=\"Data\")\n",
    "plt.plot(X_np, y_pred.detach().numpy(), \"r-\", label=\"Fitted curve\")\n",
    "plt.title(\"Data and Optimized Prediction\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Loss curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# You will notice that the fitting gets better if you run this cell multiple times, as we do not reset the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when using such naive basis functions, the final optimization result heavily depends on the initial starting guess. You can play around with more complex basis functions and different starting conditions to find a better fit. Also, play around with the learning rate and number of training epochs to get a feel for the optimization. In practise, the loss curve is a good metric for debugging when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are a class of machine learning models inspired by the structure and function of the human brain. They consist of layers of interconnected nodes (neurons) that process input data to learn patterns and make predictions. Fundamentally, neural networks are overparameterized, complex functions whose parameters are optimized using a dataset to minimize a loss function, thereby modeling the relationships in the data.\n",
    "\n",
    "In the following, we give a short, nonexhaustive introduction to NNs. Of couse, you can skip it if you are already familiar with NNs.\n",
    "\n",
    "### Why Use Neural Networks?\n",
    "Neural Networks excel in tasks involving large datasets and complex, non-linear relationships due to their:\n",
    "- **Scalability**: Handle high-dimensional inputs and large datasets effectively.\n",
    "- **Expressiveness**: Approximate any continuous function with sufficient data and capacity (Universal Approximation Theorem).\n",
    "- **Flexibility**: Adapt to tasks like regression, classification, and reinforcement learning.\n",
    "- **End-to-End Learning**: Learn directly from raw data, reducing the need for feature engineering.\n",
    "\n",
    "### Key Components of Neural Networks\n",
    "- **Input Layer**: Receives the input features (e.g., states, actions, and observations in robotics).\n",
    "- **Hidden Layers**: Extract patterns and relationships in the data. Stacking layers increases capacity.\n",
    "- **Output Layer**: Produces final predictions (e.g., next states when learning dynamics).\n",
    "- **Weights and Biases**: Parameters adjusted during training to minimize prediction error.\n",
    "- **Activation Functions**: Introduce non-linearity, enabling the model to learn complex relationships (e.g., ReLU, Sigmoid, Tanh).\n",
    "\n",
    "### Key Concepts When Training Neural Networks\n",
    "\n",
    "#### 1. **Architecture Choice**\n",
    "The architecture of a neural network defines its capacity to learn and generalize from data. Key considerations include:\n",
    "\n",
    "- **Depth and Width**: Depth and width refer to the number of layers and the number of neurons per layer, respectively. Deeper networks can learn hierarchical features but are computationally expensive and prone to vanishing gradients while wider layers increase capacity but may lead to overfitting if not regularized.\n",
    "  \n",
    "- **Activation Functions**: Introduce non-linearity to the model. Common choices include:\n",
    "  - **ReLU**: Efficient and widely used for hidden layers.\n",
    "  - **Sigmoid/Tanh**: Useful for specific tasks but prone to vanishing gradients.\n",
    "  - **Softmax**: Often used in the output layer for classification tasks.\n",
    "\n",
    "- **Layer Types**:\n",
    "  - **Fully Connected (Dense) Layers**: Standard for general-purpose tasks.\n",
    "  - **Convolutional Layers**:  Ideal for image processing tasks due to their ability to leverage the spatial structure of images. They process localized regions (receptive fields) to capture spatial features, use parameter sharing to reduce complexity, provide translation invariance for detecting features regardless of position.\n",
    "  - **Recurrent Layers**: Suitable for sequential data like time series or text (e.g., LSTMs, GRUs).\n",
    "  - **Transformer Layers**: State-of-the-art for natural language processing and vision tasks. Use self-attention mechanisms to capture long-range dependencies and relationships in the data, enabling efficient parallel processing and improved performance on sequential and spatial data.\n",
    "\n",
    "- **Task-Specific Architectures**:\n",
    "  - **ResNet**: For image classification, uses skip connections to mitigate vanishing gradients.\n",
    "  - **U-Net**: For image segmentation, combines encoder-decoder architecture with skip connections.\n",
    "  - **Transformer**: For NLP and vision tasks, excels in capturing long-range dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Enabling Generalization**\n",
    "In the end, the goal is for models to perform well on unseen data. While NNs (with suitable architecture and hyperparameter choice) generally perform well on the training data, generalization is much more difficult to achieve. Methods to achieve generalization are generally called regularization techniques. Moreover, splitting the dataset into training, test, and validation sets allows evaluating underfitting, overfitting, and hyperparameter selection.\n",
    "\n",
    "- **Detecting Underfitting and Overfitting**:\n",
    "    - **Learning Curves**: Plot training and validation losses over epochs. Converging and similar test and training losses indicates good generalization. \n",
    "  - **Underfitting**: Training and validation losses are high, indicating the model is too simple or insufficiently trained.\n",
    "  - **Overfitting**: Training loss is low, but validation loss is high, indicating the model is memorizing the training data.\n",
    "\n",
    "- **Regularization Techniques**:\n",
    "  - **Early Stopping**: Stop training when validation loss stops improving.\n",
    "  - **Dropout**: Randomly deactivate neurons during training to prevent co-adaptation.\n",
    "  - **Batch Normalization**: Normalize layer inputs to stabilize and accelerate training.\n",
    "  - **Weight Regularization**: Add L1 or L2 penalties to the loss function to constrain model complexity.\n",
    "  - **Skip Connections**: Counteracts vanishing and exploding gradients by improving gradient flow between layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Hyperparameter Optimization**\n",
    "Hyperparameters control the training process and model architecture. Optimizing them is essential for achieving strong performance.\n",
    "\n",
    "- **Common Hyperparameters**:\n",
    "  - **Learning Rate**: Controls the step size for weight updates. Too high may cause divergence; too low can slow convergence. Many optimizers (e.g., Adam) adapt the learning rate during training.\n",
    "  - **Batch Size**: Smaller batches introduce more noise but allow more frequent updates. Larger batches are more stable but require more memory.\n",
    "  - **Number of Epochs**: Specifies how many times the model sees the entire dataset.\n",
    "- **Optimization Methods**:\n",
    "  - **Grid Search**: Exhaustively searches over a predefined set of hyperparameter values, but is often inefficient.\n",
    "  - **Random Search**: Samples random combinations of hyperparameters and is generally more efficient than grid search.\n",
    "  - **Bayesian Optimization**: Uses probabilistic models to efficiently search for optimal hyperparameters.\n",
    "- **Cross-Validation**: Splits the dataset into multiple folds to evaluate model performance across different subsets, providing a more robust estimate of generalization.\n",
    "- **Validation Set**: Always use a separate validation set to evaluate hyperparameter choices.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices for Training Neural Networks\n",
    "\n",
    "1. **Normalize Input Data**: Ensure that input features have similar scales and counteract vanishing or exploding gradients.\n",
    "2. **Monitor Learning Curves**: Use training, validation, and test losses to diagnose underfitting, overfitting, and the effects of hyperparameters.\n",
    "3. **Start Simple**: Begin with a basic model and increase complexity only as needed.\n",
    "4. **Use Pretrained Models**: Leverage pretrained models when available to save time and improve performance.\n",
    "5. **Automate Hyperparameter Tuning**: Use methods like random search or Bayesian optimization to efficiently explore hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Use same data to learn a drone model as for GP\n",
    "x, y = x_drone, y_drone\n",
    "x, y = torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "x, y = x.to(device), y.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are prone to overfitting, i.e., they learn the training data including the noise really well but may fail to find the underlying feature-target relations. There exist many methods to counteract overfitting, which are a subset of Regularization methods.\n",
    "\n",
    "The most basic one is to seperate the training data into training and test sets. Although this reduces the amount of data usable for training, evaluating the learned model on the test set allows to categorize whether the model overfits.  \n",
    "\n",
    "Often the training data is further split into an additional validation set, which is often used to determine hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_torch = {\"x\": x, \"y\": y}\n",
    "data_torch[\"x_train\"], data_torch[\"x_test\"], data_torch[\"y_train\"], data_torch[\"y_test\"] = (\n",
    "    train_test_split(x, y, test_size=0.2)\n",
    ")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "# Print the shapes of the train and test datasets\n",
    "for key, value in data_torch.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common method is to normalize the input data before training and evaluation. Normalization often increases the training speed by helping to prevent vanishing or exploding gradients.\n",
    "\n",
    "Here, we are using a normalizer class that conveniently encapsulates the normalization and reverse normalization functions fitted to a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For neural networks input data should ALWAYS be normalized (also true for DRL), to prevent vanishing or exploding gradients\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(data_torch[\"x_train\"], data_torch[\"y_train\"])  # Fit on both input and output\n",
    "data_torch[\"x_train_norm\"], data_torch[\"y_train_norm\"] = normalizer.transform(\n",
    "    data_torch[\"x_train\"], data_torch[\"y_train\"]\n",
    ")\n",
    "data_torch[\"x_test_norm\"], data_torch[\"y_test_norm\"] = normalizer.transform(\n",
    "    data_torch[\"x_test\"], data_torch[\"y_test\"]\n",
    ")\n",
    "\n",
    "# Print statistics of normalized data\n",
    "print(\"Normalized Data Statistics:\")\n",
    "print(\"Mean:\", data_torch[\"x_train_norm\"].mean().item())  # should be close to zero\n",
    "print(\"Standard Deviation:\", data_torch[\"x_train_norm\"].std().item())  # should be close to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are trained in epochs and batches, which helps the optimization process. If you havent worked with Deep Learning, read up on that [here](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/). For easy data access during training, one uses [dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 2: Review and complete the CustomDataset class</h3>\n",
    "    <p>\n",
    "    Complete the <code>iter()</code> function in the <code>CustomDataset</code> in <code>utils.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CustomDataset(\n",
    "    data_torch[\"x_train_norm\"], data_torch[\"y_train_norm\"], batch_size=64, shuffle=True\n",
    ")\n",
    "val_loader = CustomDataset(\n",
    "    data_torch[\"x_test_norm\"], data_torch[\"y_test_norm\"], batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exam Preparation</h3>\n",
    "    <p>\n",
    "    Review the lecture note sections that consider Neural Networks (NNs). \n",
    "    </p>\n",
    "    <p>\n",
    "    Then understand and explain for yourself fundamental concepts of NNs and their tradeoffs:\n",
    "    <ul>\n",
    "    <li>Learning rate, epochs, batches, and mini batches</li>\n",
    "    <li>Optimizer selection (ADAM, SGD, ...)</li>\n",
    "    <li>Computational graphs in auto-differentiation</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 3: Review and complete the NeuralNetwork class</h3>\n",
    "    <p>Review the class <code>NeuralNetwork</code> in <code>neural_network.py</code>.</p>\n",
    "    <p>Then complete the <code>__init__()</code> function that initializes the Neural Network (NN) and defines the model architecture. You have complete freedom over the architecture choice (e.g., you can select the activation functions, number of layers, hidden units per layer, and layer types) as long as:</p>\n",
    "    <ul>\n",
    "        <li>The model uses layer types defined by <code>torch.nn</code>.</li>\n",
    "        <li>The model has at most <code>1e6</code> trainable parameters.</li>\n",
    "        <li>All model parameters are included in the hyperparameter dictionary defined in the below code cell. </li>\n",
    "        <li>The input and output dimensions are not changed.</li>\n",
    "    </ul>\n",
    "    <p>Note: You can adjust the <code>forward()</code> function if your architecture requires it.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed for reproducibility\n",
    "set_seed(42)\n",
    "# Define hyperparameters for the neural network (make sure to adjust them according to your implementation)\n",
    "# TODO A hidden dim of 5 is not sufficient.\n",
    "hyperparameters = {\n",
    "    \"input_dim\": data_torch[\"x_train_norm\"].shape[1],\n",
    "    \"output_dim\": data_torch[\"y_train_norm\"].shape[1],\n",
    "    \"hidden_dims\": [5, 5],\n",
    "}\n",
    "# Create Neural Network\n",
    "model = NeuralNetwork(hyperparameters)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Plot predictions before training\n",
    "x_train_plot = data_torch[\"x_train_norm\"].clone().detach().cpu().numpy()\n",
    "y_train_plot = data_torch[\"y_train_norm\"].clone().detach().cpu().numpy()\n",
    "y_pred_plot = model(data_torch[\"x_train_norm\"]).detach().cpu().numpy()\n",
    "\n",
    "# index zero corresponds to x position. If we plot x_pos (current state) against x_pos (next state), we expect an almost linear graph.\n",
    "feature_idx = 0\n",
    "\n",
    "sort_indices = np.argsort(x_train_plot[:, 0])\n",
    "x_sorted = x_train_plot[sort_indices, 0]\n",
    "y_true_sorted = y_train_plot[sort_indices, feature_idx]\n",
    "y_pred_sorted = y_pred_plot[sort_indices, feature_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_sorted, y_true_sorted, alpha=0.6, label=\"Ground Truth\", color=\"blue\")\n",
    "plt.scatter(x_sorted, y_pred_sorted, alpha=0.6, label=\"Predictions (Before Training)\", color=\"red\")\n",
    "plt.title(\"Neural Network Predictions vs Ground Truth (Before Training)\")\n",
    "plt.xlabel(\"Input Feature (First Dimension, x_pos)\")\n",
    "plt.ylabel(f\"Output Feature {state_names[feature_idx]}\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 4: Review and complete the RegressionTrainer class</h3>\n",
    "    Review the class <code>RegressionTrainer</code> in <code>neural_network.py</code>.\n",
    "    <p>\n",
    "    First, implement the <code>get_optimizer_criterion_scheduler()</code> function which initializes the optimizer, loss function, and a learning rate scheduler. You can choose any optimizer and loss function from <code>torch.optim</code> and <code>torch.nn</code>, respectively. Pass needed paramters via the \"cfg\" dictionary.\n",
    "    </p>\n",
    "    <p>\n",
    "    Then implement the <code>train_epoch()</code> function that performs a training iteration.\n",
    "    </p>\n",
    "    <p>\n",
    "    The <code>train()</code> function saves a checkpoint at the end of the training which is used to test your implementation. Make sure to commit the checkpoint before submitting your solution!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise04.neural_network import RegressionTrainer # noqa: I001\n",
    "# Train network\n",
    "nn_results = {}\n",
    "#TODO: Define cfg dictionary\n",
    "cfg = {\n",
    "    \"optimizer\": {},\n",
    "    \"scheduler\": {},\n",
    "    \"criterion\": {},\n",
    "}\n",
    "trainer = RegressionTrainer(model, cfg=cfg, device=device)\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train(train_loader, val_loader, epochs=40, patience=5)\n",
    "\n",
    "nn_results[\"t_train\"] = time.time() - start_time\n",
    "\n",
    "# Plot learning curve. Understanding learning curves is a crucial part to debug your models: https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\n",
    "plot_learning_curves(trainer.train_losses, trainer.val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, debugging learning curves is important in understanding where your deep learning algorithms fails. Check out [this blog post](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) for a good overview of how to debug learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions after training\n",
    "x_train_plot = data_torch[\"x_train_norm\"].clone().detach().cpu().numpy()\n",
    "y_train_plot = data_torch[\"y_train_norm\"].clone().detach().cpu().numpy()\n",
    "y_pred_plot = model(data_torch[\"x_train_norm\"]).detach().cpu().numpy()\n",
    "\n",
    "# Choose \"pos_x\" again\n",
    "feature_idx = 0\n",
    "\n",
    "sort_indices = np.argsort(x_train_plot[:, 0])\n",
    "x_sorted = x_train_plot[sort_indices, 0]\n",
    "y_true_sorted = y_train_plot[sort_indices, feature_idx]\n",
    "y_pred_sorted = y_pred_plot[sort_indices, feature_idx]\n",
    "\n",
    "\n",
    "plot_regression_results(x_sorted, y_true_sorted, y_pred_sorted, title=\"Neural Network Predictions vs Ground Truth (After Training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set and make sure that test loss is similar to train loss (otherwise we overfit or there are other problems during training or in the data)\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "with torch.no_grad():  # no gradient\n",
    "    nn_results[\"y_pred_norm\"] = model(data_torch[\"x_test_norm\"])\n",
    "nn_results[\"t_pred\"] = time.time() - start_time\n",
    "\n",
    "\n",
    "test_loss = trainer.criterion(nn_results[\"y_pred_norm\"], data_torch[\"y_test_norm\"])\n",
    "print(f\"Train Loss: {trainer.train_losses[-1]:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Note: as those losses are calculated on normalized data, they are not directly interpretable as physical quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same statistics as for the GP\n",
    "_, nn_results[\"y_pred\"] = normalizer.inverse_transform(\n",
    "    data_torch[\"x_test_norm\"], nn_results[\"y_pred_norm\"]\n",
    ")\n",
    "nn_results[\"y_pred\"] = nn_results[\"y_pred\"].cpu().numpy()\n",
    "nn_results[\"y_std\"] = np.zeros_like(\n",
    "    nn_results[\"y_pred\"]\n",
    ")  # (This) Neural network do not provide uncertainty estimates\n",
    "\n",
    "# Calculate absolute error and squared error\n",
    "nn_results[\"mae\"], nn_results[\"rmse\"], nn_results[\"uncertainty\"] = error_statistics(\n",
    "    data_torch[\"y_test\"].cpu().numpy(), nn_results[\"y_pred\"], nn_results[\"y_std\"], model_name=\"NN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions of two states with standard deviation for a random single point\n",
    "plot_2d_positions_with_std(\n",
    "    data_torch[\"y_train\"].cpu().numpy(),\n",
    "    data_torch[\"y_test\"].cpu().numpy(),\n",
    "    nn_results[\"y_pred\"],\n",
    "    y_std=None,\n",
    "    state_1_name=\"pos_x\",\n",
    "    state_2_name=\"pos_z\",\n",
    "    show_train=False,\n",
    "    x_lim=(-0.4, -0.1),  # for better visibility\n",
    "    y_lim=(0.5, 0.6),  # for better visibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison\n",
    "Now we compare the performance of the Neural Network and Gaussian Process implementations.\n",
    "For that, we evaluate the accuracy of the predictions for the test set and additionally measure the times required for inference and learning.\n",
    "\n",
    "To allow for a fair comparison, we will reimplement the GP model using [GPyTorch](https://gpytorch.ai/). GPyTorch is a package build upon PyTorch that implements a wide range of GP models and utility tools that  allow for GPU acceleration. Thereby the GP parameters are learned similarly as for NNs.\n",
    "\n",
    "Note: GP implementations have generally a much higher memory footprint than a equivalent NN implementation. In practice this means that training an exact GP is on cpu is limited to datasets of size $\\leq 2000$ increasing to $\\approx 10000$ datapoints when using gpu. Hence, we will use [Stochastic Varational GP Regression](https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/SVGP_Regression_CUDA.html) for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 5: Review the SVGPTrainer class and complete the IndependentMultitaskSVGPModel </h3>\n",
    "    <p>Review the classes <code>SVGPTrainer</code> and <code>IndependentMultitaskSVGPModel</code> in <code>gaussian_process.py</code>.</p>\n",
    "    <p> Complete the two code sections in the <code>__init__()</code> function, which set up first variational strategy and then the mean and convariance modules. </p>\n",
    "    For more details and hints check the information on the GPyTorch documentation (https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/index.html)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\" # for now we will use CPU\n",
    "gp_trainer = SVGPTrainer(X_train=raw_data[\"x_train\"], y_train=raw_data[\"y_train\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_trainer.train(lr=1e-3, epochs=250, log_interval=50)\n",
    "svgp_results = {}\n",
    "svgp_results[\"t_train\"] = gp_trainer.training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svgp_results[\"y_pred\"], svgp_results[\"y_std\"] = (\n",
    "    gp_trainer.infer(X_test=raw_data[\"x_test\"])\n",
    ")\n",
    "svgp_results[\"t_pred\"] = gp_trainer.inference_time\n",
    "svgp_results[\"mae\"], svgp_results[\"rmse\"], svgp_results[\"uncertainty\"] = error_statistics(\n",
    "    raw_data[\"y_test\"], svgp_results[\"y_pred\"], svgp_results[\"y_std\"], model_name=\"SVGP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the table\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Method\": [\"Manual GP\", \"SVGP\", \"Neural Network\"],\n",
    "    \"Mean Absolute Error\": [gp_results[\"mae\"], svgp_results[\"mae\"], nn_results[\"mae\"]],\n",
    "    \"Mean RMSE\": [gp_results[\"rmse\"], svgp_results[\"rmse\"], nn_results[\"rmse\"]],\n",
    "    \"Mean Uncertainty\": [\n",
    "        gp_results[\"uncertainty\"],\n",
    "        svgp_results[\"uncertainty\"],\n",
    "        nn_results[\"uncertainty\"],\n",
    "    ],\n",
    "    \"Training Time (s)\": [gp_results[\"t_train\"], svgp_results[\"t_train\"], nn_results[\"t_train\"]],\n",
    "    \"Inference Time (s)\": [gp_results[\"t_pred\"], svgp_results[\"t_pred\"], nn_results[\"t_pred\"]],\n",
    "}\n",
    "\n",
    "# Create and print the results table\n",
    "results_df = pd.DataFrame(data)\n",
    "print(results_df)\n",
    "\n",
    "# Plot the error distributions for all three models\n",
    "plot_error_distribution(raw_data[\"y_test\"], gp_results[\"y_pred\"], state_names, model_name=\"GP\")\n",
    "plot_error_distribution(raw_data[\"y_test\"], svgp_results[\"y_pred\"], state_names, model_name=\"SVGP\")\n",
    "plot_error_distribution(\n",
    "    data_torch[\"y_test\"].cpu().numpy(), nn_results[\"y_pred\"], state_names, model_name=\"NN\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
