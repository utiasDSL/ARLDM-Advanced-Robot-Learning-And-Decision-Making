{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: LQR & ILQR\n",
    "\n",
    "> Welcome to the Advanced Robot Learning and Decision Making exercises!\n",
    "\n",
    "In this exercise you will learn how to develop your own LQR and ILQR algorithms to stabilize the drone to a certain position. \n",
    "\n",
    "\n",
    "\n",
    "1. **Implement discrete-time, infinite-horizon LQR for hovering**\n",
    "    \n",
    "    Interface the Gymnasium Environment with the Controller, Linearize and Discretize the System, LQR control gains, and stabilize drone at target position.\n",
    "\n",
    "2. **Implement ILQR for hovering**\n",
    "\n",
    "    Linearize the System, Approximation of Costs, Policy Update, Implement backward pass, and stabilize drone at target position.\n",
    "\n",
    "3. **Compare LQR and ILQR, and practical considerations.**\n",
    "\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "##### Recap\n",
    "\n",
    "The state $x$ of this robot is defined by the position of its center of mass and the body orientations, as well as their derivatives. The inputs $u$ that control the system are the vertical thrust $F_z$ and the desired roll, pitch, and yaw angles $(\\phi_d, \\theta_d, \\psi_d)$ around each axis of a coordinate frame fixed to the quadrotor body ([attitude control interface](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/functional-areas/sensor-to-control/controllers/)).\n",
    "\n",
    "The continuous-time non-linear system dynamics in its general form are written down as\n",
    "\n",
    "$$\n",
    "\\dot{x} = f(x) + g(x)\\cdot u,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "x & y & z &  \\phi & \\theta & \\psi & \\dot{x} & \\dot{y} & \\dot{z} & \\dot{\\phi} & \\dot{\\theta} & \\dot{\\psi}\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "u = [F_z, \\phi_d, \\theta_d, \\psi_d]^T\n",
    "$$\n",
    "\n",
    "This system is underactuated with two degrees of underactuation—it has twelve states in total (or six degrees of freedom) and four independent actuation inputs. We implement this system in simulation using JAX ([crazyflow](https://github.com/utiasDSL/crazyflow)), as well as in a symbolic model using [CasADi](https://web.casadi.org/), as demonstrated in the previous exercise.\n",
    "\n",
    "_____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from crazyflow.constants import GRAVITY, MASS\n",
    "from ilqr import ILQR\n",
    "from lqr import LQR\n",
    "from utils import obs_to_state\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make_vec(\n",
    "    \"DroneReachPos-v0\",\n",
    "    num_envs=1,\n",
    "    freq=500,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "print(\"observation space: \\n\", env.observation_space)\n",
    "print(\"action space: \\n\", env.action_space)\n",
    "print(\"time step:\", 1 / env.sim.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Recap: Action Space</h3>\n",
    "    <p>The action space is a continuous space defined by a Box object with a shape of <b>(1, 4)</b>. The range for the first variable is <b>0.11264675 to 0.5933658</b>, while the other three variables range from <b>-π/2 to π/2</b>. The actions are represented as <b>float32</b>.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 LQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 State Vector Representation\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 1: Reformat the Observation</h3>\n",
    "    <p>\n",
    "      Please implement the function <code>obs_to_state(obs)</code> in <code>exercise02/utils.py</code>. We need to reformat the dict observation returned by the simulation into a numpy array required by our controller:\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "x & y & z &  \\phi & \\theta & \\psi & \\dot{x} & \\dot{y} & \\dot{z} & \\dot{\\phi} & \\dot{\\theta} & \\dot{\\psi}\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "   - $x$: Position in the x-direction.\n",
    "   - $y$: Position in the y-direction.\n",
    "   - $z$: Position in the z-direction.\n",
    "   - $\\phi$: Roll angle.\n",
    "   - $\\theta$: Pitch angle.\n",
    "   - $\\psi$: Yaw angle.\n",
    "   - $\\dot{x}$: Velocity in the x-direction (x\\_dot).\n",
    "   - $\\dot{y}$: Velocity in the y-direction (y\\_dot).\n",
    "   - $\\dot{z}$: Velocity in the z-direction (z\\_dot).\n",
    "   - $\\dot{\\phi}$: Roll angular velocity (phi\\_dot).\n",
    "   - $\\dot{\\theta}$: Pitch angular velocity (theta\\_dot).\n",
    "   - $\\dot{\\psi}$: Yaw angular velocity (psi\\_dot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Linearization of system dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the last exercise, we can easily obtain a CasADi symbolic model with very similar behavior to the simulation using <a href=\"https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/sim/symbolic.py#L291C5-L291C22\"><code>symbolic_from_sim</code></a>. As we require the symbolic model for the controllers, this method is already being called for you in the <code>BaseController.get_symbolic</code> base class, and is available via the <code>model</code> property. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 2: Check Code</h3>\n",
    "   <p>  Read the <code>__init__</code>, <code>setup_model</code> and <code>setup_linearization</code> methodes of the <a href=\"https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/sim/symbolic.py#L27\"><code>SymbolicModel</code></a> class in <a href=\"https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/sim/symbolic.py\"><code>crazyflow/sim/symbolic.py</code></a>. Understand how the symbolic representations of the drone’s dynamics, state, and cost functions are created. To understand this, you need some familarity with <a href=\"https://web.casadi.org/\">CasADi</a>. You will need to interact with those definitions in the following tasks.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the symbolic model for LQR, you first need to linearize the system. We denote the linear system with\n",
    "\n",
    "$$\n",
    "\\dot{x} = A x + B u.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 3: Linearization</h3>\n",
    "    <p>\n",
    "      Go to <code>compute_lqr_gain</code> function in <code>exercise02/lqr.py</code>. Using the <code>symbolic model</code> to compute the linearized system matrices <code>A</code> and <code>B</code> at the equilibrium. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discrete-time LQR controller requires discretized, linearized system dynamics $(A_k, B_k)$, whereas the function you implemented above still provides continuous, linearized system dynamics $(A, B)$. Next, we need to discretize the linearized dynamics.\n",
    "\n",
    "\n",
    "A simple approximation between the two is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\delta \\dot{x}(t) &= A \\delta x(t) + B \\delta u(t)  \\\\\n",
    "\\frac{\\delta x_{k+1} - \\delta x_k}{\\delta t} &= A \\delta x_k + B \\delta u_k  \\\\\n",
    "\\delta x_{k+1} &= (I + A \\delta t) \\delta x_k + (B \\delta t) \\delta u_k,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "as also stated in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 4: Discretization</h3>\n",
    "    <p>\n",
    "        Go to <code>exercise02/utils.py</code>, and finish the implementation of the function <code>discretize_linear_system</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Quadratic cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infinite-horizon discrete-time quadratic cost function is given by:\n",
    "\n",
    "$$\n",
    "J(\\bm{x_0}) = \\sum_{k=0}^{\\infty} \\left( \\bm{(x_k - x_{ref})}^T \\bm{Q} \\bm{(x_k - x_{ref})} + \\bm{u_k}^T \\bm{R} \\bm{u_k} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 5: Exam Preparation: Equilibrium and Cost Matrices</h3>\n",
    "    <p>\n",
    "        In the following cell, the equilibrium and weight matrices have been defined. You may start with the default values and later feel free to adjust the weight matrices.  \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating point\n",
    "x_op = np.array([0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # State equilibrium\n",
    "u_op = np.array([MASS * GRAVITY, 0.0, 0.0, 0.0], dtype=np.float32)  # Control equilibrium\n",
    "\n",
    "# TODO later on in the exercise, experiment here with different values in the Q and R matrices.\n",
    "# 5 for position, 0.1 for orientation, 2 for roll, 3 for velocity, 1 for angular velocity\n",
    "q_diag = [5, 5, 5, 0.1, 0.1, 2, 3, 3, 3, 1, 1, 1]\n",
    "r_diag = [1, 1, 1, 1]\n",
    "Q_lqr = np.diag(q_diag)  # State cost\n",
    "R_lqr = np.diag(r_diag)  # Control cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 LQR controller gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the LQR problem is obtained by solving the **Discrete Algebraic Riccati Equation (DARE):**\n",
    "\n",
    "$$\n",
    "\\bm{S} = \\bm{Q} + \\bm{A}^T \\bm{S} \\bm{A} - (\\bm{A}^T \\bm{S} \\bm{B})(\\bm{R} + \\bm{B}^T \\bm{S} \\bm{B})^{-1} (\\bm{B}^T \\bm{S} \\bm{A})\n",
    "$$\n",
    "\n",
    "The optimal feedback control policy is given by:\n",
    "\n",
    "$$\n",
    "\\bm{K} = \\left( \\bm{R} + \\bm{B}^T \\bm{S} \\bm{B} \\right)^{-1} \\left( \\bm{B}^T \\bm{S} \\bm{A} \\right) \\\\\n",
    "\\bm{u^*} = -\\bm{K} \\bm{(x_k - x_{ref})}\n",
    "$$\n",
    "\n",
    "The implementation will follow three key steps:\n",
    "\n",
    "1. **Obtain the system matrices**: Retrieve the $\\bm{A}$ and $\\bm{B}$ matrices from the discrete-time system dynamics;  \n",
    "   *Hint*: you may use the method `discretize_linear_system()` to get the discretized system matrices  \n",
    "\n",
    "2. **Solve for the Hessian matrix $\\bm{S}$ of optimal cost**: Compute the solution to the Discrete Algebraic Riccati Equation (DARE) to obtain $\\bm{S}$;  \n",
    "   *Hint*: you may use the method `scipy.linalg.solve_discrete_are()` to solve DARE  \n",
    "\n",
    "3. **Calculate the feedback gain $\\bm{K}$**: Use the computed Hessian $\\bm{S}$ to determine the LQR feedback gain $\\bm{K}$;  \n",
    "   *Hint*: you may use the method `numpy.linalg.inv()` to solve the inverse matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 6: LQR Gain</h3>\n",
    "    <p>\n",
    "        Go to <code>exercise02/lqr.py</code>, and finish the implementation of the function <code>compute_lqr_gain</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lqr_controller = LQR(env, x_op, u_op, Q_lqr, R_lqr)\n",
    "gain_lqr = lqr_controller.gain\n",
    "\n",
    "print(\"shape of gain:\", gain_lqr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Regulate the drone to the target state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 7: Control Input</h3>\n",
    "    <p>\n",
    "        Please go to <code>exercise02/lqr.py</code> and implement the <code>step_control</code>. Then, run the following cell. Feel free to change the <code>goal</code>. Keep in mind that we test your implementation using different target goals on submission. If you are able to stabilize to the original goal provided by us, you should be fine.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can use the `Tab`-key to switch between world and body camera in mujoco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=SEED)\n",
    "state = obs_to_state(obs)\n",
    "# print(obs)\n",
    "# Step through the environment\n",
    "pos_log = [obs[\"pos\"].squeeze()]\n",
    "control_input_log = []\n",
    "fps = 60\n",
    "\n",
    "# TODO: you can play around with the lqr_goal state\n",
    "lqr_goal = np.array([0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # set lqr_goal state\n",
    "\n",
    "env.unwrapped._goal = env.unwrapped._goal.at[...].set(\n",
    "    np.array([lqr_goal[0], lqr_goal[1], lqr_goal[2]])\n",
    ") \n",
    "\n",
    "for i in range(2500):\n",
    "    control_input = lqr_controller.step_control(state, lqr_goal)\n",
    "\n",
    "    control_input_log.append(control_input.flatten())\n",
    "    obs, reward, terminated, truncated, info = env.step(control_input)\n",
    "    pos_log.append(obs[\"pos\"].squeeze())\n",
    "    state = obs_to_state(obs)\n",
    "    # print('state:',state)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended at step:\", i)\n",
    "        break\n",
    "\n",
    "    if (i * fps) % env.sim.freq < fps:\n",
    "        env.render()\n",
    "        env.unwrapped.sim.viewer.viewer.cam.lookat = env.unwrapped._goal[0]\n",
    "        time.sleep(1 / fps)\n",
    "\n",
    "env.sim.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time array based on fixed step interval\n",
    "dt = 1 / env.sim.freq\n",
    "pos_log = np.array(pos_log)\n",
    "time_log = np.arange(len(pos_log)) * dt\n",
    "\n",
    "# Plot theta and control input vs. time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_log, pos_log[:, 0], label=\"x\", color=\"blue\")\n",
    "plt.plot(time_log, pos_log[:, 1], label=\"y\", color=\"green\")\n",
    "plt.plot(time_log, pos_log[:, 2], label=\"z\", color=\"red\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.title(\"Position vs Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the drone was stabilized at the desired position!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_input_values = np.array(control_input_log)  # shape: (steps, 4)\n",
    "\n",
    "time_log = np.arange(control_input_values.shape[0]) * dt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_log, control_input_values[:, 0], label=\"normalized thrust [N] (LQR)\", color=\"blue\")\n",
    "plt.plot(\n",
    "    time_log, control_input_values[:, 1], label=\"desired roll angle [rad] (LQR)\", color=\"orange\"\n",
    ")\n",
    "plt.plot(\n",
    "    time_log, control_input_values[:, 2], label=\"desired pitch angle [rad] (LQR)\", color=\"green\"\n",
    ")\n",
    "plt.plot(time_log, control_input_values[:, 3], label=\"desired yaw angle [rad] (LQR)\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Control Input\")\n",
    "plt.title(\"Control Input over Time (lqr)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the begining, the thrusts reached the maximum thrust of the motor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 8: Exam Preparation</h3>\n",
    "    <p>\n",
    "    How does the result change as we vary Qs and Rs?\n",
    "    </p>\n",
    "    <p>\n",
    "    Submission is not required, but it’s crucial for the exam.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 9: Exam Preparation</h3>\n",
    "    <p>\n",
    "    How does the controller performance vary when the initial state changes and why?\n",
    "    </p>\n",
    "    <p>\n",
    "    Submission is not required, but it’s crucial for the exam.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 10: Exam Preparation</h3>\n",
    "    <p>\n",
    "    How could you adapt the existing LQR controller implementation for tracking a moving target, instead of a static one? What has to change? What do you need to think about?\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ILQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now design an ILQC controller for the quadrotor reach-the-goal problem. We initialize the ILQC algorithm with the LQR control law obtained above and iteratively update the control law to minimize the prescribed cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialization of control policy\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_k(x_k) & = \\bar{u}_k + \\delta u_k^* \\\\\n",
    "           & = \\bar{u}_k - gain * \\delta x_k \\\\\n",
    "           & = \\bar{u}_k - gain * (x_k - \\bar{x}_k) \\\\\n",
    "           & = \\underbrace{\\bar{u}_k + gain * \\bar{x}_k}_{\\text{feedforward } u_{k,\\text{ff}}} \n",
    "            \\underbrace{ - gain * x_k}_{\\text{feedback}} \\\\\n",
    "           & = u_{k,\\text{ff}} + gain_{fb} * x_k.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired state\n",
    "ilqr_goal = np.array([0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # goal state\n",
    "\n",
    "nx = 12  # dimension of state vector\n",
    "nu = 4  # dimension of input vector\n",
    "\n",
    "input_ff = u_op + gain_lqr.dot(ilqr_goal)\n",
    "gains_fb = -gain_lqr\n",
    "\n",
    "input_ff = np.tile(input_ff.reshape(nu, 1), (1, 2500))\n",
    "gains_fb = np.tile(gains_fb.reshape(1, nu, nx), (2500, 1, 1))\n",
    "\n",
    "print(\"Shape of input_ff:\", input_ff.shape)\n",
    "print(\"Shape of gains_fb:\", gains_fb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that, in each ILQC iteration, we first perform a forward pass and roll out the states and inputs based on the control law from the previous iteration. We denote the sequence of rollout states as $ \\{\\bar{x}_k\\}_{k=0}^N $ and the sequence of inputs as $ \\{\\bar{u}_k\\}_{k=0}^{N-1} $, where $ N $ is the number of simulation time steps. In the questions below, we will derive the key elements of the backward pass for updating the control law in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you need to linearize and discretize the system about an operating point $(\\bar{x}_k, \\bar{u}_k)$. The goal is to write a function that outputs the matrices of the linearized system for a given operating point $(\\bar{x}_k, \\bar{u}_k)$:\n",
    "\n",
    "$$ A_k, B_k = \\text{drone\\_linear}(\\bar{x}_k, \\bar{u}_k, \\delta t, \\bar{v})$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 11: Linearized  and Discretized System Dynamics</h3>\n",
    "    <p>\n",
    "        Linearize and discretize the system dynamic on an operating point. Finish the implementation of the function <code> dynamic_lin_disc</code> in <code> exercise02/ilqr.py</code> :\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-discretize the cost function and express the cost function in the following form:\n",
    "\n",
    "$$\n",
    "J(x_0) = g_N (x_N) + \\delta t \\sum_{k=0}^{N-1} g_k(x_k, u_k), \\tag{5}\n",
    "$$\n",
    "\n",
    "where $g_N(x_N)$ is the terminal cost and $g_k(x_k, u_k)$ is the stage cost. Derive second-order approximations of the terminal cost and the stage cost about $(\\bar{x}_k, \\bar{u}_k)$. The approximated costs should take the following forms:\n",
    "\n",
    "$$\n",
    "g_N(x_N) \\approx \\bar{g}_N + q_N^\\top \\delta x_N + \\frac{1}{2} \\delta x_N^\\top Q_N \\delta x_N, \\tag{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_k(x_k, u_k) \\approx \\bar{g}_k + q_k^\\top \\delta x_k + r_k^\\top \\delta u_k + \\frac{1}{2} \\delta x_k^\\top Q_k \\delta x_k + \\frac{1}{2} \\delta u_k^\\top R_k \\delta u_k + \\delta u_k^\\top P_k \\delta x_k. \\tag{7}\n",
    "$$\n",
    "\n",
    "Write functions that compute the terminal and the stage costs in Eqns. (6) and (7):\n",
    "\n",
    "$$\n",
    "\\bar{g}_N, q_N, Q_N = \\text{terminal\\_cost\\_quad}(Q_t, x_{\\text{goal}}, \\bar{x}_N)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{g}_k, q_k, Q_k, r_k, R_k, P_k = \\text{stage\\_cost\\_quad}(Q_s, R_s, x_{\\text{goal}}, \\delta t, \\bar{x}_k, \\bar{u}_k)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 12: Cost Approximation</h3>\n",
    "    <p>\n",
    "        Finish the implementation of the functions <code> terminal_cost_quad</code> and <code> stage_cost_quad</code> in <code> exercise02/ilqr.py</code>. (The suffix *_quat.py is for 'quadratic'.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman Equation (BE) for the value function at time step $ k $ can be written as:\n",
    "\n",
    "$$\n",
    "J^*(x_k) = \\min_u \\left( g_k(x_k, u_k) + J^*(x_{k+1}) \\right). \\tag{8}\n",
    "$$\n",
    "\n",
    "Assuming the following approximation of the value function $ J^*(x_{k+1}) $:\n",
    "\n",
    "$$\n",
    "J^*(x_{k+1}) \\approx \\bar{s}_{k+1} + \\delta x_{k+1}^\\top s_{k+1} + \\frac{1}{2} \\delta x_{k+1}^\\top S_{k+1} \\delta x_{k+1}, \\tag{9}\n",
    "$$\n",
    "\n",
    "and given the approximate stage cost $ g_k(x_k, u_k) $ in Eqn. (7), derive an expression for the optimal $ \\delta u_k^* $ at each time step. Express the updated control law $ u_k = \\bar{u}_k + \\delta u_k^* $ in the following form:\n",
    "\n",
    "$$\n",
    "u_k = \\theta_{k,ff} + \\theta_{k,fb} x_k,\n",
    "$$\n",
    "\n",
    "where $ \\theta_{k,ff} \\in \\mathbb{R}^m $ and $ \\theta_{k,fb} \\in \\mathbb{R}^{m \\times n} $.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 13: Exam Preparation: Derivation</h3>\n",
    "    <p>\n",
    "    Derive an expression for the optimal incremental policy. \n",
    "    </p>\n",
    "    <p>\n",
    "    Submission is not required, but it’s crucial for the next code task.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 14: Exam Preparation</h3>\n",
    "    <p>\n",
    "    Consider the following equations for updating <code>\\bar{s}_k, s_k, S_k</code>. But how should we initialize them? \n",
    "    </p>\n",
    "    <p>\n",
    "    Submission is not required, but it’s crucial for code task and exam.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "The equations for updating $ \\bar{s}_k $, $ s_k $, $ S_k $ from $ \\bar{s}_{k+1} $, $ s_{k+1} $, $ S_{k+1} $ at each time step are:\n",
    "\n",
    "$$\n",
    "\\bar{s}_k = \\bar{g}_k + \\bar{s}_{k+1} + \\frac{1}{2} \\delta u_{k,ff}^{*^\\top} H_k \\delta u_{k,ff}^* + \\delta u_{k,ff}^{*^\\top} l_k,\n",
    "$$\n",
    "\n",
    "$$\n",
    "s_k = q_k + A_k^\\top s_{k+1} + K_k^\\top H_k \\delta u_{k,ff}^* + K_k^\\top l_k + G_k^\\top \\delta u_{k,ff}^*,\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_k = Q_k + A_k^\\top S_{k+1} A_k + K_k^\\top H_k K_k + K_k^\\top G_k + G_k^\\top K_k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 15: Policy Update</h3>\n",
    "    <p>\n",
    "        Finish the implementation of the function <code> update_policy</code> in <code> exercise02/ilqr.py</code>, to update the control law and coefficients of the approximation of the value function at time step k.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Iteratively update policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design cost weight matrices. Feel free to adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_diag = [5, 5, 5, 0.1, 0.1, 2, 3, 3, 3, 1, 1, 1]\n",
    "r_diag = [1, 1, 1, 1]\n",
    "Q_ilqr = np.diag(q_diag)  # State cost weight\n",
    "R_ilqr = np.diag(r_diag)  # Input cost weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 16: Complete ILQR</h3>\n",
    "    <p>\n",
    "        You have already implemented the key components of the backward pass of ILQR algorithm. Please go to <code> learn()</code> in <code>exercise02/ilqr.py</code> and complete the implementation of the ILQR algorithm.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you finish the Task, please run the next cell. It will take some time. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilqr_controller = ILQR(env, Q_ilqr, R_ilqr, ilqr_goal, input_ff, gains_fb)\n",
    "\n",
    "ilqr_controller.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset(seed=SEED)\n",
    "state = obs_to_state(obs)\n",
    "\n",
    "env.unwrapped._goal = env.unwrapped._goal.at[...].set(np.array([ilqr_goal[0], ilqr_goal[1], ilqr_goal[2]])) # update goal in the environment for vizualization\n",
    "\n",
    "pos_log_ilqr = [obs[\"pos\"].squeeze()]\n",
    "control_input_log_ilqr = []\n",
    "\n",
    "# Simulation loop\n",
    "for i in range(2500):  # Simulate for 2500 steps\n",
    "    # envs.render()\n",
    "\n",
    "    # Compute control action (force) using the iLQR gain\n",
    "    control_input = ilqr_controller.step_control(\n",
    "        state, i\n",
    "    )  # gains_fb[:, i].dot(state) + input_ff[i]\n",
    "\n",
    "    # Clip the control input to the specified range\n",
    "    control_input = np.clip(control_input, env.action_space.low, env.action_space.high)\n",
    "\n",
    "    # Convert to np.ndarray\n",
    "    action = control_input.reshape(1, 4).astype(np.float32)  # (1, 4)\n",
    "\n",
    "    # Take a step in the environment with the computed action\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    state = obs_to_state(obs)\n",
    "\n",
    "    # Log data\n",
    "    pos_log_ilqr.append(obs[\"pos\"].squeeze())\n",
    "    control_input_log_ilqr.append(action.flatten())\n",
    "\n",
    "    # Check if the episode is terminated\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended at step:\", i)\n",
    "        break\n",
    "\n",
    "    if (i * fps) % env.sim.freq < fps:\n",
    "        env.render()\n",
    "        env.unwrapped.sim.viewer.viewer.cam.lookat = env.unwrapped._goal[0]\n",
    "        time.sleep(1 / fps)\n",
    "# Close the environment\n",
    "env.sim.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 17: Exam Preparation</h3>\n",
    "    <p>\n",
    "    Try different initial conditions and plot the state and input. Comment on the difference between the LQR controller and the ILQR controller.\n",
    "    </p>\n",
    "    <p>\n",
    "    E.g. [0,0,8,0,8,0,0,0,0,0,0,0]\n",
    "    </p>\n",
    "    <p>\n",
    "    Submission is not required, but it’s crucial for the exam.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 18: Exam Preparation</h3>\n",
    "    <p>\n",
    "    How would the controllers' performence change when there is a mismatch between the model and the real system?\n",
    "    </p>\n",
    "    <p>\n",
    "    Submission is not required, but it’s crucial for the exam.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time array based on fixed step interval\n",
    "pos_log_ilqr = np.array(pos_log_ilqr)\n",
    "time_log = np.arange(len(pos_log_ilqr)) * dt\n",
    "\n",
    "# Plot theta and control input vs. time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_log, pos_log_ilqr[:, 0], label=\"x(iLQR)\", color=\"blue\")\n",
    "plt.plot(time_log, pos_log_ilqr[:, 1], label=\"y(iLQR)\", color=\"green\")\n",
    "plt.plot(time_log, pos_log_ilqr[:, 2], label=\"z(iLQR)\", color=\"red\")\n",
    "plt.plot(time_log, pos_log[:, 0], label=\"x(LQR)\", color=\"blue\", linestyle=\"--\")\n",
    "plt.plot(time_log, pos_log[:, 1], label=\"y(LQR)\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(time_log, pos_log[:, 2], label=\"z(LQR)\", color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"position\")\n",
    "plt.title(\"position vs Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_input_values_ilqr = np.array(control_input_log_ilqr)  # shape: (steps, 4)\n",
    "\n",
    "time_log = np.arange(control_input_values.shape[0]) * dt\n",
    "\n",
    "# Set up plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define labels, colors, and control inputs\n",
    "control_labels = [\n",
    "    (\"normalized thrust [N]\", \"blue\"),\n",
    "    (\"desired roll angle [rad]\", \"orange\"),\n",
    "    (\"desired pitch angle [rad]\", \"green\"),\n",
    "    (\"desired yaw angle [rad]\", \"red\")\n",
    "]\n",
    "\n",
    "# Plot iLQR control inputs\n",
    "for i, (label, color) in enumerate(control_labels):\n",
    "    plt.plot(time_log, control_input_values_ilqr[:, i], label=f\"{label} (iLQR)\", color=color)\n",
    "\n",
    "# Plot LQR control inputs with dashed lines\n",
    "for i, (label, color) in enumerate(control_labels):\n",
    "    plt.plot(time_log, control_input_values[:, i], label=f\"{label} (LQR)\", color=color, linestyle=\"--\")\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Control Input\")\n",
    "plt.title(\"Control Input over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further techniques on model and simulation based lqr techniques, please refer to [Reist et al, Simulation-Based LQR-Trees with Input and State Constraints](https://groups.csail.mit.edu/robotics-center/public_papers/Reist10.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
